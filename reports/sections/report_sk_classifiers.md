Задача классификации в машинном обучении — это задача отнесения объекта к одному из заранее определенных классов на основании его формализованных признаков. Каждый из объектов в этой задаче представляется в виде вектора в N-мерном пространстве, каждое измерение в котором представляет собой описание одного из признаков объекта.
Для распознования цифр в работе используются классификаторы из библиотеки **skikit-learn**.

### Классификатор kNN

Для классификации каждого из объектов тестовой выборки необходимо последовательно выполнить следующие операции:
Вычислить расстояние до каждого из объектов обучающей выборки.
Отобрать k объектов обучающей выборки, расстояние до которых минимально
Класс классифицируемого объекта — это класс, наиболее часто встречающийся среди k ближайших соседей

**kNN** — один из простейших алгоритмов классификации, поэтому на реальных задачах он зачастую оказывается неэффективным.
Помимо точности классификации, проблемой этого классификатора является скорость классификации: если в обучающей выборке N объектов, в тестовой выборе M объектов, а размерность пространства — K, то количество операций для классификации тестовой выборки может быть оценено как O(K*M*N).

### Дерево принятия решений

Суть алгоритма заключается в том, что на каждой итерации делается случайная выборка переменных, после чего,
на этой новой выборке запускают построение дерева принятия решений.
При этом производится “bagging” — выборка случайных двух третей наблюдений для обучения,
а оставшаяся треть используется для оценки результата. Такую операцию проделывают сотни или тысячи раз.
Результирующая модель будет результатом “голосования” набора полученных при моделировании деревьев.


### Бинарное дерево решений

Допустим у нас есть какой-то очень слабый алгоритм, скажем, дерево принятия решений. 
Если мы сделаем очень много разных моделей с использованием этого слабого алгоритма и усредним результат их предсказаний,
то итоговый результат будет существенно лучше. 
Это, так называемое, обучение ансамбля в действии. 
Алгоритм **Random Forest** потому и называется "Случайный Лес", 
для полученных данных он создает множество деревьев приятия решений и потом усредняет результат их предсказаний. 
Важным моментом тут является элемент случайности в создании каждого дерева. 
Ведь понятно, что если мы создадим много одинаковых деревьев, то результат их усреднения будет обладать точностью одного дерева.



